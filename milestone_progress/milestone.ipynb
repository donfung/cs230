{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "milestone.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "G7XOKahaEvp2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "f38cbb51-0a08-4ff1-a8a2-218a80bcaf99"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W5lycwTeEwvM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Implementing a object recognition and tracking algorithm \n",
        "\n",
        "1. Use YOLO -> generate bounding box tags [This uses pre-trained weights, so eventually might need retraining]\n",
        "2. Given bouding box data from the algorithm, instantiate a tracker by calling the sort algorithm method, which tracks and collapses trackers frame to frame based on YOLO output data.\n",
        "3. Save tracker outputs in text format specified by MOT205 \n",
        "4. From the MOT2015 data set, run python implementation of metric calculator to evaluate performace for differnt test sets \n",
        "\n",
        "TO DO:\n",
        "1. YOLO needs fixes -> slightly buggy right now, but generates bounding boxes fine\n",
        "2. Replace tracker (SORT/KALMAN) with RNN based tracker (on-going)\n",
        "3. Revaluate benchmark performance after 1&2 \n",
        "\n",
        "\n",
        "Dependencie\n",
        "filterpy, cv2, numpy, torch, "
      ]
    },
    {
      "metadata": {
        "id": "q_qVkhEdJNXw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import filterpy\n",
        "from sort import *\n",
        "from models import *\n",
        "from utils_ import *\n",
        "\n",
        "import os, sys, time, datetime, random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C8AvY4smJxuv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Configre YOLO weights and classes \n",
        "\n",
        "config_path='/content/drive/My Drive/Colab Notebooks/object_tracking/pytorch_objectdetecttrack/config/yolov3.cfg'\n",
        "weights_path='/content/drive/My Drive/Colab Notebooks/object_tracking/pytorch_objectdetecttrack/yolov3.weights'\n",
        "class_path='/content/drive/My Drive/Colab Notebooks/object_tracking/pytorch_objectdetecttrack/config/coco.names'\n",
        "img_size=416\n",
        "conf_thres=0.8\n",
        "nms_thres=0.4\n",
        "\n",
        "# Load model and weights\n",
        "# defines a YOLO instantiation using the Darknet flow (found easily on Github)\n",
        "\n",
        "model = Darknet(config_path, img_size=img_size)\n",
        "model.load_weights(weights_path)\n",
        "model.cuda()\n",
        "model.eval()\n",
        "classes = load_classes(class_path)\n",
        "Tensor = torch.cuda.FloatTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "30ohgyrNKMqT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define a track image function to extract box data for a single image\n",
        "# We will use this again and again to do frame by frame object detection \n",
        "\n",
        "##### Detect Image Function #################\n",
        "def detect_image(img):\n",
        "    # scale and pad image\n",
        "    ratio = min(img_size/img.size[0], img_size/img.size[1])\n",
        "    imw = round(img.size[0] * ratio)\n",
        "    imh = round(img.size[1] * ratio)\n",
        "    img_transforms = transforms.Compose([ transforms.Resize((imh, imw)),\n",
        "         transforms.Pad((max(int((imh-imw)/2),0), max(int((imw-imh)/2),0), max(int((imh-imw)/2),0), max(int((imw-imh)/2),0)),\n",
        "                        (128,128,128)),\n",
        "         transforms.ToTensor(),\n",
        "         ])\n",
        "    # convert image to Tensor\n",
        "    image_tensor = img_transforms(img).float()\n",
        "    image_tensor = image_tensor.unsqueeze_(0)\n",
        "    input_img = Variable(image_tensor.type(Tensor))\n",
        "    # run inference on the model and get detections\n",
        "    with torch.no_grad():\n",
        "        detections = model(input_img)\n",
        "        detections = non_max_suppression(detections, 80, conf_thres, nms_thres)\n",
        "    return detections[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tMNlCsrWKshz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# SORT + YOLO \n",
        "# Set input data (video) path & output (video) path \n",
        "\n",
        "# in our case , we ran this after mounting on my collab acoount \n",
        "\n",
        "videopath = '/content/drive/My Drive/Colab Notebooks/object_tracking/pytorch_objectdetecttrack/Videos/Venice-2.mp4'\n",
        "savepath = '/content/drive/My Drive/Colab Notebooks/object_tracking/pytorch_objectdetecttrack'\n",
        "\n",
        "\n",
        "%pylab inline \n",
        "from IPython.display import clear_output\n",
        "\n",
        "cmap = plt.get_cmap('tab20b')\n",
        "colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]\n",
        "\n",
        "\n",
        "\n",
        "# initialize Sort object and video capture\n",
        "from sort import *\n",
        "vid = cv2.VideoCapture(videopath)\n",
        "mot_tracker = Sort() \n",
        "\n",
        "\n",
        "\n",
        "# Section to create a cv2 instance to handle video input and process it frame by \n",
        "# frame \n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "ret,frame=vid.read()\n",
        "vw = frame.shape[1]\n",
        "vh = frame.shape[0]\n",
        "print (\"Video size\", vw,vh)\n",
        "outvideo = cv2.VideoWriter(videopath.replace(\".mp4\", \"-det.mp4\"),fourcc,20.0,(vw,vh))\n",
        "\n",
        "\n",
        "frames = 0\n",
        "starttime = time.time()\n",
        "obj_id = 0 \n",
        "\n",
        "with open('%s/output/det.txt'%(savepath),'w') as out_file:\n",
        "  for ii in range(599):\n",
        "      frames += 1\n",
        "      ret, frame = vid.read()\n",
        "      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "      pilimg = Image.fromarray(frame)\n",
        "      detections = detect_image(pilimg)\n",
        "\n",
        "      img = np.array(pilimg)\n",
        "      pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
        "      pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
        "      unpad_h = img_size - pad_y\n",
        "      unpad_w = img_size - pad_x\n",
        "\n",
        "      if detections is not None:  \n",
        "          tracked_objects = mot_tracker.update(detections.cpu())\n",
        "          unique_labels = detections[:, -1].cpu().unique()\n",
        "          n_cls_preds = len(unique_labels)\n",
        "          for x1, y1, x2, y2, obj_id, cls_pred in tracked_objects:\n",
        "              if (int(cls_pred==0)):\n",
        "                box_h = int(((y2 - y1) / unpad_h) * img.shape[0])\n",
        "                box_w = int(((x2 - x1) / unpad_w) * img.shape[1])\n",
        "                y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])\n",
        "                x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])\n",
        "\n",
        "                color = colors[int(obj_id) % len(colors)]\n",
        "                color = [i * 255 for i in color]\n",
        "                cls = classes[int(cls_pred)]\n",
        "                cv2.rectangle(frame, (x1, y1), (x1+box_w, y1+box_h), color, 4)\n",
        "                cv2.rectangle(frame, (x1, y1-35), (x1+len(cls)*19+60, y1), color, -1)\n",
        "                #cv2.putText(frame, cls + \"-\" + str(int(obj_id)), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 3)\n",
        "                cv2.putText(frame, cls, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 3)\n",
        "  \n",
        "      #fig=figure(figsize=(12, 8))\n",
        "      #title(\"Video Stream\")\n",
        "      #imshow(frame)\n",
        "      #show()\n",
        "      #clear_output(wait=True)\n",
        "      outvideo.write(frame)\n",
        "      \n",
        "      for d in tracked_objects: \n",
        "        if(int(cls_pred==0)):\n",
        "          print('%d,%d,%.2f,%.2f,%.2f,%.2f,1,-1,-1,-1'%(ii,d[4],d[0],d[1],d[2]-d[0],d[3]-d[1]),file=out_file) \n",
        "            \n",
        "      ch = 0xFF &cv2.waitKey(1)\n",
        "      if ch == 27:\n",
        "        break\n",
        "   \n",
        "  \n",
        "totaltime = time.time()-starttime\n",
        "print(frames, \"frames\", totaltime/frames, \"s/frame\")\n",
        "#cv2.destroyAllWindows()\n",
        "outvideo.release()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}